{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d258878a",
   "metadata": {},
   "source": [
    "This file involves the broader data aspect of this work. It can be run or the pickles created can be used. For future iterations of NUMBAT this code may need modification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fca739",
   "metadata": {},
   "source": [
    "## Start by importing the NUMBAT dataset and required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f16e1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb1f801",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from convertbng.util import convert_bng, convert_lonlat\n",
    "import branca\n",
    "import branca.colormap\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "from math import atan2, degrees\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4d318",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3bb95",
   "metadata": {},
   "source": [
    "Load the NUMBAT data into a dictionary, data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95af0ce1",
   "metadata": {},
   "source": [
    "For subsequent years extend the range of \"year\" and adjust \"days\" appropriately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117af847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this to reflect the folder the NUMBAT dataset is stored on your machine\n",
    "folder = \"~/NUMBAT-main/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e36960ba",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT16FRI_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT16MON_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT16TWT_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT17FRI_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT17MON_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT17TWT_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT18MON_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT18TWT_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT19MON_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT19TWT_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT20MON_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT20TWT_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT21MON_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT21TWT_Outputs.xlsx\n",
      "this year and day combination doesn't exist~/Desktop/dis/Numbat/NBT22MTT_Outputs.xlsx\n"
     ]
    }
   ],
   "source": [
    "#Initialize the dictionary\n",
    "data_dict = {}\n",
    "#Set the years and day of the week types we care about\n",
    "year = range(16,23)\n",
    "days = [\"MTT\", \"SAT\", \"SUN\", \"FRI\", \"MON\", \"TWT\"]\n",
    "\n",
    "# Iterate over the list of filenames and import each file\n",
    "for yr in year:\n",
    "    for day in days:\n",
    "        \n",
    "        #Use a try-except function so different naming conventions don't cause errors\n",
    "        try:\n",
    "            #Add each file in NUMBAT as a dataframe to the dictionary\n",
    "            file_path = folder + 'NBT{}{}_Outputs.xlsx'.format(yr, day)\n",
    "            data = pd.read_excel(file_path, sheet_name='Link_Loads',skiprows=2, header=0)\n",
    "            data_dict['data{}{}'.format(yr, day)] = data\n",
    "            \n",
    "        #Print combinations that cause a failure to ensure the failures are as expected\n",
    "        except:\n",
    "            print(\"this year and day combination doesn't exist\" + file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b57933",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37440f36",
   "metadata": {},
   "source": [
    "### Adding Station Coordinates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805b66e",
   "metadata": {},
   "source": [
    "Aquire coordinates from open street maps (www.openstreetmap.org) and add it to the NUMBAT data frames in data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565fa826",
   "metadata": {},
   "source": [
    "This function uses geopy to get latitude and longitude coordinates based on station name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1c55b1",
   "metadata": {
    "code_folding": [
     0,
     5,
     31,
     35,
     39,
     43,
     51,
     55,
     59,
     63,
     67,
     71,
     75,
     79
    ]
   },
   "outputs": [],
   "source": [
    "#This takes a NUMBAT station name and gets its coordinates\n",
    "\n",
    "#Create a nominatim object\n",
    "geolocator = Nominatim(user_agent=\"agent\")\n",
    "\n",
    "def get_lat_long(station_name):\n",
    "    \n",
    "    #Remove added words that keep coordinates from being found with geopy\n",
    "    station_name =station_name.replace(\" TfL\", \"\")\n",
    "    station_name =station_name.replace(\" TFL\", \"\")\n",
    "    \n",
    "    #For Kensington Olympia\n",
    "    station_name =station_name.replace(\"(Olympia)\", \"Olympia\")\n",
    "\n",
    "    \n",
    "    #store original station name\n",
    "    og = station_name\n",
    "    \n",
    "    #Remove added words and symbols that keep coordinates from being found with geopy\n",
    "    station_name = re.sub('\\(.*\\)','',station_name, flags=re.DOTALL)\n",
    "    station_name = re.sub(' LO','',station_name, flags=re.DOTALL)\n",
    "    station_name =station_name.replace(\" LU\", \"\")\n",
    "    station_name=re.sub(' LO','',station_name, flags=re.DOTALL)\n",
    "    station_name=station_name.replace(\" EL\", \"\")\n",
    "    station_name=station_name.replace(\" Trams\", \"\")\n",
    "    station_name=station_name.replace(\" NR\", \"\")\n",
    "    \n",
    "    #Bank and Monument Station is better known as Bank Station\n",
    "    station_name=station_name.replace(\"Bank and Monument\", \"Bank\")\n",
    "    \n",
    "    #manually add coordinates for stations that are hard for geopy to understand\n",
    "    if station_name == \"Watford\":\n",
    "        lat = 51.65763\n",
    "        long =  -0.417519\n",
    "        \n",
    "    elif station_name == \"Church Tram London\":\n",
    "        lat = 51.373730\n",
    "        long =   -0.104555\n",
    "    \n",
    "    elif og == \"avenue road tram stop\":\n",
    "        lat = 51.406625\n",
    "        long =    -0.049583\n",
    "        \n",
    "    elif og == \"Avenue Road\":\n",
    "        lat = 51.406625\n",
    "        long =    -0.049583\n",
    "       \n",
    "    elif og == \"Langley Berks\":\n",
    "        lat = 51.508065\n",
    "        long =     -0.541365\n",
    "        \n",
    "    elif og == \"Langley\":\n",
    "        lat = 51.508065\n",
    "        long =     -0.541365\n",
    "        \n",
    "    elif og == \"Beckenham Road\":\n",
    "        lat = 51.4097588\n",
    "        long =     -0.0432697\n",
    "        \n",
    "    elif og == \"Arena\":\n",
    "        lat = 51.391297\n",
    "        long =     -0.058328\n",
    "        \n",
    "    elif og == \"Woodside\":\n",
    "        lat = 51.387653\n",
    "        long =    -0.064389\n",
    "        \n",
    "    elif og == \"Blackhorse Lane\":\n",
    "        lat = 51.385035\n",
    "        long =    -0.070186       \n",
    "        \n",
    "    elif station_name == \"Church Street\":\n",
    "        lat = 51.373730\n",
    "        long =   -0.104555\n",
    "        \n",
    "    elif station_name == \"Croxley\":    \n",
    "        lat = 51.647218\n",
    "        long =   -0.441725\n",
    "        \n",
    "    elif station_name == \"Moor Park\":\n",
    "        lat = 51.630077\n",
    "        long =    -0.431931\n",
    "        \n",
    "    #try to get the coordinates from geopy\n",
    "    else:\n",
    "        try:\n",
    "            location = geolocator.geocode(station_name + \" station, London\")\n",
    "            lat = location.latitude\n",
    "            long = location.longitude\n",
    "\n",
    "        except:\n",
    "            \n",
    "            try:\n",
    "                station_name = station_name + \" tram stop, London\"\n",
    "                location = geolocator.geocode(station_name)\n",
    "                lat = location.latitude\n",
    "                long = location.longitude\n",
    "\n",
    "            except:\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    location = geolocator.geocode(og + \" station, UK\")\n",
    "                    lat = location.latitude\n",
    "                    long = location.longitude\n",
    "                    \n",
    "                #if there is a mistake add the original name to a list, \"mistake\"\n",
    "                except:\n",
    "                    lat = None\n",
    "                    long = None\n",
    "                    mistakes.append(og)\n",
    "\n",
    "    return lat, long\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05dcca5",
   "metadata": {},
   "source": [
    "### This is the original method to add coordinate data without using extra data from TfL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9cf36",
   "metadata": {},
   "source": [
    "Initialize an empty set to store the unique station values for get lat long then create a dictionary to get the lat long coords for every station. This intermediate step reduces time consuming api calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "abd95ae0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Add the coordinates as values to a dictionary with stations as keys\n",
    "latlongcoords = {}\n",
    "\n",
    "#initialize a list to store mistakes\n",
    "mistakes = []\n",
    "\n",
    "#Initialize a set to store all unique station names, use a set to avoid uneccessary duplication\n",
    "unique_stations = set()\n",
    "\n",
    "# Iterate over the list of filenames and import each file\n",
    "for df in data_dict.values():\n",
    "    \n",
    "    # Add the unique values to the set\n",
    "    unique_stations.update(df['From Station'].unique())\n",
    "\n",
    "# Convert the set to a list or other iterable object if needed\n",
    "unique_list = list(unique_stations)\n",
    "\n",
    "#Add the coordinates of each station as values to a dictionary with the station name as a key\n",
    "for a in unique_list:\n",
    "    latlongcoords[a] = get_lat_long(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae1e0e",
   "metadata": {},
   "source": [
    "Use dictionary to add coordinates to each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6a41c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for data in data_dict.values():\n",
    "    data[\"StartLat\"]= data[\"From Station\"].apply(lambda x: latlongcoords.get(x)[0])\n",
    "    data[\"StartLong\"] = data[\"From Station\"].apply(lambda x: latlongcoords.get(x)[1])\n",
    "    data[\"StopLat\"]= data[\"To Station\"].apply(lambda x: latlongcoords.get(x)[0])\n",
    "    data[\"StopLong\"] = data[\"To Station\"].apply(lambda x: latlongcoords.get(x)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dd9858",
   "metadata": {},
   "source": [
    "### Add 2022 MTT and new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00042fe2",
   "metadata": {},
   "source": [
    "Add a 2022 MTT by combinging MON with TWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cde74655",
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "df22MON = data_dict.get(\"data22MON\")\n",
    "df22TWT = data_dict.get(\"data22TWT\")\n",
    "\n",
    "# Assuming df22MON and df22TWT are loaded from data_dict as you did\n",
    "# Initialize df22MTT as an empty DataFrame with the same structure as df22MON\n",
    "df22MTT = pd.DataFrame(columns=df22MON.columns)\n",
    "\n",
    "# Apply the weighted average formula for the passenger volume columns\n",
    "for col in df22MON.columns[10:113]:\n",
    "    df22MTT[col] = (df22MON[col] + 3 * df22TWT[col]) / 4\n",
    "\n",
    "# Optionally, if you need to copy the rest of the columns from df22MON or df22TWT\n",
    "for col in df22MON.columns[:10].union(df22MON.columns[113:]):\n",
    "    df22MTT[col] = df22MON[col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f66aa5c",
   "metadata": {},
   "source": [
    "Add new columns to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a7c0e28",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#Create a function to adjust collumns for each individual NUMBAT dataframe\n",
    "def add_new_columns(df):\n",
    "    df['Normalized Total'] = df['Total'] / df['Total'].sum()\n",
    "    df['Normalized AM Peak'] = df['AM Peak   '] / df['AM Peak   '].sum()\n",
    "    df['Normalized PM Peak'] = df['PM Peak   '] / df['PM Peak   '].sum()\n",
    "    \n",
    "    #divide by 2 cuz adding 2 normalized columns\n",
    "    df['Normalized AMPM Peak'] = (df['AM Peak   '] + df['PM Peak   '])/(df['AM Peak   '].sum() + df['PM Peak   '].sum())\n",
    "    \n",
    "    \n",
    "    df['Normalized Not Peak'] = (df['Total'] - df['AM Peak   '] - df['PM Peak   ']) / (df['Total'].sum() - df['AM Peak   '].sum() - df['PM Peak   '].sum())\n",
    "    df['AMPM Peak'] = df['PM Peak   '] + df['AM Peak   ']\n",
    "    df[\"Not Peak\"] = df['Total']-df['AMPM Peak']\n",
    "    \n",
    "    #remove spaces\n",
    "    df = df.rename(columns={'PM Peak   ':\"PM Peak\", 'AM Peak   ':\"AM Peak\", 'Early     ': \"Early\"\n",
    "                           , 'Midday    ': \"Midday\", 'Evening   ': 'Evening',\n",
    "       'Late      ':'Late'})\n",
    "\n",
    "    \n",
    "    #add Early AM Peak and Normal AM Peak \n",
    "    df[\"EarlyMorning\"] = df.iloc[:,15:21].sum(numeric_only=True,axis=1)\n",
    "    df[\"LateMorning\"] = df.iloc[:,21:27].sum(numeric_only=True,axis=1)\n",
    "    return df\n",
    "\n",
    "#Apply the function to every value in data_dict\n",
    "data_dict = {key: add_new_columns(value) for key, value in data_dict.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a02a9",
   "metadata": {},
   "source": [
    "## Add Colour Scheme "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4f7b4",
   "metadata": {},
   "source": [
    "The colour scheme comes from https://blog.tfl.gov.uk/2022/12/22/digital-colour-standard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c546763",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"~/NUMBAT-main/data/\"\n",
    "colours = pd.read_csv(directory + \"colours.csv\", usecols=lambda x: x != 'Unnamed: 0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ecb0a9c",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#This function converts between both nomenclatures \n",
    "def LineConverter(line):\n",
    "    line = line.replace(\"Line\", \"line\")\n",
    "    line = line.replace(\"London Trams\", \"Tram\")\n",
    "    if \"LO\" in line:\n",
    "        line = \"Overground\"\n",
    "        \n",
    "    if \"H&C\" in line:\n",
    "        line = \"Hammersmith & City\"\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d18e965e",
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "df1 = colours\n",
    "\n",
    "#Iterate over all the NUMBAT data and add line colour\n",
    "for key in data_dict.keys():\n",
    "    \n",
    "        # Get the dataframe corresponding to the current key\n",
    "    df2 = data_dict.get(key)\n",
    "\n",
    "    #add a new column to match NUMBAT line formatting with TfL Colour Scheme Line name formatting\n",
    "    df2[\"Adj Line\"] = df2[\"Line\"].map(LineConverter)\n",
    "    \n",
    "        # Merge df1 and df2 based on line, using a right join\n",
    "    joined_df = pd.merge(colours, df2, left_on=['Name'], right_on=['Adj Line'], how='right')\n",
    "\n",
    "    #drop unecessary columns\n",
    "    joined_df.drop(['Name', \"Adj Line\"], axis=1, inplace=True)\n",
    "\n",
    "        # Update the value of the current key in the data_dict dictionary with the joined_df dataframe\n",
    "    data_dict[key] = joined_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a50e5d",
   "metadata": {},
   "source": [
    "## Add Naptan Data from TfL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c834d5b",
   "metadata": {},
   "source": [
    "This data is from contacting TfL\n",
    "It is missing the Battersea and Nine Elms extension\n",
    "It contains NaPTAN Codes, Atco Codes, Common Name, NLC, ALC codes, Easterling and Notherlings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fbf2180",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "directory = \"~/NUMBAT-main/data/\"\n",
    "nap = pd.read_excel(directory + \"ASC-NAPTAN table.xlsx\")\n",
    "asc = pd.read_excel(directory + \"ASC-NAPTAN table.xlsx\",sheet_name=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90588d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MasterNLC</th>\n",
       "      <th>MasterASC</th>\n",
       "      <th>UniqueStationName</th>\n",
       "      <th>PrimaryNaptanStopArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>750</td>\n",
       "      <td>ABRd</td>\n",
       "      <td>Abbey Road</td>\n",
       "      <td>940GZZDLABR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MasterNLC MasterASC UniqueStationName PrimaryNaptanStopArea\n",
       "0        750      ABRd        Abbey Road           940GZZDLABR"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad96aa",
   "metadata": {},
   "source": [
    "Use convert_lonlat package to convert from Easting and Northing to Longitude and Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f80eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nap = nap.dropna(subset=['Easting'])\n",
    "\n",
    "#add Longitude and Latitude coordinates\n",
    "nap[\"Lon\"] = convert_lonlat(nap[\"Easting\"],nap[\"Northing\"])[0]\n",
    "nap[\"Lat\"] = convert_lonlat(nap[\"Easting\"], nap[\"Northing\"])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a445278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StopAreaCode</th>\n",
       "      <th>AtcoCode</th>\n",
       "      <th>TfL_mASC</th>\n",
       "      <th>TiplocCode</th>\n",
       "      <th>PrimaryStopArea</th>\n",
       "      <th>mASC_Matches</th>\n",
       "      <th>CommonName</th>\n",
       "      <th>Easting</th>\n",
       "      <th>Northing</th>\n",
       "      <th>StopType</th>\n",
       "      <th>AdministrativeAreaCode</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Description</th>\n",
       "      <th>NodeType</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>910GALEXNDP</td>\n",
       "      <td>9100ALEXNDP</td>\n",
       "      <td>AAPr</td>\n",
       "      <td>ALEXNDP</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Alexandra Palace Rail Station</td>\n",
       "      <td>530295.0</td>\n",
       "      <td>190460.0</td>\n",
       "      <td>RLY</td>\n",
       "      <td>110</td>\n",
       "      <td>Rail</td>\n",
       "      <td>Rail Station</td>\n",
       "      <td>AccessArea</td>\n",
       "      <td>-0.12021</td>\n",
       "      <td>51.597925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StopAreaCode     AtcoCode TfL_mASC TiplocCode  PrimaryStopArea  \\\n",
       "0  910GALEXNDP  9100ALEXNDP     AAPr    ALEXNDP             True   \n",
       "\n",
       "   mASC_Matches                     CommonName   Easting  Northing StopType  \\\n",
       "0             1  Alexandra Palace Rail Station  530295.0  190460.0      RLY   \n",
       "\n",
       "   AdministrativeAreaCode  Mode   Description    NodeType      Lon        Lat  \n",
       "0                     110  Rail  Rail Station  AccessArea -0.12021  51.597925  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nap.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b3bb2",
   "metadata": {},
   "source": [
    "Join the data from TfL with the NUMBAT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a8901b6",
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "df1 = asc\n",
    "\n",
    "#Iterate over all the NUMBAT data\n",
    "for key in data_dict.keys():\n",
    "    \n",
    "    # Get the dataframe corresponding to the current key\n",
    "    df2 = data_dict.get(key)\n",
    "\n",
    "    # Merge df1 and df2 based on specific column matching, using a right join\n",
    "    joined_df = pd.merge(df1, df2, left_on=['MasterNLC', 'MasterASC'], right_on=['From NLC', 'From ASC'], how='right')\n",
    "\n",
    "    # Reset column names of df1 in the joined_df dataframe\n",
    "    joined_df = joined_df.rename(columns={'PrimaryNaptanStopArea': 'From PrimaryNaptanStopArea', 'UniqueStationName': 'From UniqueStationName'})\n",
    "\n",
    "    # Define a list of columns to be dropped from the joined_df dataframe\n",
    "    matching_columns = ['MasterNLC', 'MasterASC']\n",
    "\n",
    "    # Drop the matching columns from the joined_df dataframe\n",
    "    joined_df.drop(matching_columns, axis=1, inplace=True)\n",
    "\n",
    "    # Merge df1 and joined_df based on another set of column matching, using a right join\n",
    "    joined_df = pd.merge(df1, joined_df, left_on=['MasterNLC', 'MasterASC'], right_on=['To NLC', 'To ASC'], how='right')\n",
    "\n",
    "    # Reset column names of df1 in the joined_df dataframe\n",
    "    joined_df = joined_df.rename(columns={'PrimaryNaptanStopArea': 'To PrimaryNaptanStopArea', 'UniqueStationName': 'To UniqueStationName'})\n",
    "\n",
    "    # Define a list of columns to be dropped from the joined_df dataframe\n",
    "    matching_columns = ['MasterNLC', 'MasterASC']\n",
    "\n",
    "    # Drop the matching columns from the joined_df dataframe\n",
    "    joined_df.drop(matching_columns, axis=1, inplace=True)\n",
    "\n",
    "    # Update the value of the current key in the data_dict dictionary with the joined_df dataframe\n",
    "    data_dict[key] = joined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912e0c2",
   "metadata": {},
   "source": [
    "Find out what is missing from ASC; its Barking Riverside', 'Canary Wharf EL', 'Custom House EL' as of 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96656f55",
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "missing = pd.DataFrame()\n",
    "df1 = asc\n",
    "\n",
    "for key in data_dict.keys():\n",
    "    \n",
    "    df2 = data_dict.get(key)\n",
    "    \n",
    "    joined_df = pd.merge(df1, df2, left_on=['MasterNLC', 'MasterASC'], right_on=['From NLC', 'From ASC'], how='right')\n",
    "\n",
    "    matching_columns = ['MasterNLC', 'MasterASC']\n",
    "    joined_df.drop(matching_columns, axis=1, inplace=True)\n",
    "    \n",
    "    # Filter the unmatched rows\n",
    "    unmatched_rows = joined_df[joined_df['UniqueStationName'].isnull()]\n",
    "\n",
    "    # Print the unmatched rows\n",
    "    unmatched_df = pd.DataFrame(unmatched_rows)\n",
    "\n",
    "    # Concatenate unmatched_df with missing\n",
    "    missing = pd.concat([missing, unmatched_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24ec5f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Barking Riverside', 'Canary Wharf EL', 'Custom House EL'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(missing[\"From Station\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e9a0d0e",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#Fix issue of Elizabeth line stations being given seperate names from DLR\n",
    "#Barking Riverside is new 'Canary Wharf EL' and 'Custom House EL' are the same station as Canary Wharf and Custom House\n",
    "\n",
    "for key in data_dict.keys():\n",
    "\n",
    "    df = data_dict.get(key)\n",
    "\n",
    "    # Iterate through the rows of the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if To UniqueStationName or To PrimaryNaptanStopArea are missing\n",
    "        if pd.isna(row['To UniqueStationName']) or pd.isna(row['To PrimaryNaptanStopArea']):\n",
    "            # Modify the station name by removing \" El\"\n",
    "            modified_name = row['To Station'].replace(\" EL\", \"\")\n",
    "            # Search for the modified station name in the DataFrame and fill in the missing values\n",
    "            match = df[df['To Station'] == modified_name]\n",
    "            if not match.empty:\n",
    "                df.at[index, 'To UniqueStationName'] = match['To UniqueStationName'].values[0]\n",
    "                df.at[index, 'To PrimaryNaptanStopArea'] = match['To PrimaryNaptanStopArea'].values[0]\n",
    "           \n",
    "            modified_name = row['To Station'].replace(\" EL\", \" DLR\")\n",
    "            # Search for the modified station name in the DataFrame and fill in the missing values\n",
    "            match = df[df['To Station'] == modified_name]\n",
    "            if not match.empty:\n",
    "                df.at[index, 'To UniqueStationName'] = match['To UniqueStationName'].values[0]\n",
    "                df.at[index, 'To PrimaryNaptanStopArea'] = match['To PrimaryNaptanStopArea'].values[0]\n",
    "\n",
    "                \n",
    "        # Check if From UniqueStationName or From PrimaryNaptanStopArea are missing\n",
    "        if pd.isna(row['From UniqueStationName']) or pd.isna(row['From PrimaryNaptanStopArea']):\n",
    "            # Modify the station name by removing \" el\"\n",
    "            modified_name = row['From Station'].replace(\" EL\", \"\")\n",
    "            # Search for the modified station name in the DataFrame and fill in the missing values\n",
    "            match = df[df['From Station'] == modified_name]\n",
    "            if not match.empty:\n",
    "                df.at[index, 'From UniqueStationName'] = match['From UniqueStationName'].values[0]\n",
    "                df.at[index, 'From PrimaryNaptanStopArea'] = match['From PrimaryNaptanStopArea'].values[0]\n",
    "            \n",
    "            modified_name = row['From Station'].replace(\" EL\", \" DLR\")\n",
    "            # Search for the modified station name in the DataFrame and fill in the missing values\n",
    "            match = df[df['From Station'] == modified_name]\n",
    "            if not match.empty:\n",
    "                df.at[index, 'From UniqueStationName'] = match['From UniqueStationName'].values[0]\n",
    "                df.at[index, 'From PrimaryNaptanStopArea'] = match['From PrimaryNaptanStopArea'].values[0]\n",
    "\n",
    "  \n",
    "    # Add the modified DataFrame\n",
    "    data_dict[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5470d3e",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform the join operation on nap and data_dict\n",
    "df1 = nap\n",
    "\n",
    "\n",
    "\n",
    "for key in data_dict.keys():\n",
    "    \n",
    "    # Get the dataframe corresponding to the current key\n",
    "    df2 = data_dict.get(key)\n",
    "\n",
    "    # Merge df1 and df2 based on specific column matching, using a right join\n",
    "    joined_df = pd.merge(df1, df2, left_on=['StopAreaCode', 'TfL_mASC'], right_on=['To PrimaryNaptanStopArea', 'To ASC'], how='right')\n",
    "\n",
    "    # Reset column names of df1 in the joined_df dataframe\n",
    "    joined_df = joined_df.rename(columns={'AtcoCode': 'To AtcoCode', 'CommonName': 'To CommonName', \"TiplocCode\" : \"To TiplocCode\",\n",
    "                                         \"Easting\": \"To Easting\", \"Northing\": \"To Northing\", \"AdministrativeAreaCode\": \"To AdministrativeAreaCode\"\n",
    "                                         , \"StopType\": \"To StopType\", \"Lon\": \"StopLong\", \"Lat\": \"StopLat\"})\n",
    "\n",
    "    # Define a list of columns to be dropped from the joined_df dataframe\n",
    "    matching_columns = ['StopAreaCode', 'TfL_mASC', \"mASC_Matches\", \"PrimaryStopArea\", \"Mode\", \"Description\", \"NodeType\"]\n",
    "\n",
    "    # Drop the matching columns from the joined_df dataframe\n",
    "    joined_df.drop(matching_columns, axis=1, inplace=True)\n",
    "\n",
    "    # Merge df1 and joined_df based on another set of column matching, using a right join\n",
    "    joined_df = pd.merge(df1, joined_df, left_on=['StopAreaCode', 'TfL_mASC'], right_on=['From PrimaryNaptanStopArea', 'From ASC'], how='right')\n",
    "\n",
    "    # Reset column names of df1 in the joined_df dataframe\n",
    "    joined_df = joined_df.rename(columns={'AtcoCode': 'From AtcoCode', 'CommonName': 'From CommonName', \"TiplocCode\" : \"From TiplocCode\",\n",
    "                                         \"Easting\": \"From Easting\", \"Northing\": \"From Northing\", \"AdministrativeAreaCode\": \"From AdministrativeAreaCode\"\n",
    "                                         , \"StopType\": \"From StopType\", \"Lon\": \"StartLong\", \"Lat\": \"StartLat\"})\n",
    "\n",
    "    # Drop the matching columns from the joined_df dataframe\n",
    "    joined_df.drop(matching_columns, axis=1, inplace=True)\n",
    "\n",
    "    # Update the value of the current key in the data_dict dictionary with the joined_df dataframe\n",
    "    data_dict[key] = joined_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bb253",
   "metadata": {},
   "source": [
    "Find out what is missing from nap; its the Battersea Expansion and probably Elizabeth line in future years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d11ca3d7",
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "missing = pd.DataFrame()\n",
    "df1 = nap\n",
    "\n",
    "for key in data_dict.keys():\n",
    "    \n",
    "    df2 = data_dict.get(key)\n",
    "    \n",
    "    joined_df = pd.merge(df1, df2, left_on=['StopAreaCode', 'TfL_mASC'], right_on=['To PrimaryNaptanStopArea', 'To ASC'], how='right')\n",
    "\n",
    "    matching_columns = [ \"mASC_Matches\", \"PrimaryStopArea\", \"Mode\", \"Description\", \"NodeType\"]\n",
    "    joined_df.drop(matching_columns, axis=1, inplace=True)\n",
    "    \n",
    "    # Filter the unmatched rows\n",
    "    unmatched_rows = joined_df[joined_df['StartLong'].isnull()]\n",
    "\n",
    "    # Print the unmatched rows\n",
    "    unmatched_df = pd.DataFrame(unmatched_rows)\n",
    "\n",
    "    # Concatenate unmatched_df with missing\n",
    "    missing = pd.concat([missing, unmatched_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "784538a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Barking Riverside', 'Battersea Power Station', 'Canary Wharf EL',\n",
       "       'Custom House EL', 'Nine Elms', 'Woolwich EL'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(missing[\"From Station\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc794a2e",
   "metadata": {
    "code_folding": [
     0,
     3
    ]
   },
   "outputs": [],
   "source": [
    "#Fix issue of Elizabeth line stations being given seperate names from DLR\n",
    "#Barking Riverside is new 'Canary Wharf EL' and 'Custom House EL' are the same station as Canary Wharf and Custom House\n",
    "\n",
    "for key in data_dict.keys():\n",
    "\n",
    "    df = data_dict.get(key)\n",
    "\n",
    "    # Iterate through the rows of the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if To UniqueStationName or To PrimaryNaptanStopArea are missing\n",
    "        if pd.isna(row['To UniqueStationName']) or pd.isna(row['To PrimaryNaptanStopArea']):\n",
    "            # Modify the station name by removing \" El\"\n",
    "            modified_name = row['To Station'].replace(\" EL\", \"\")\n",
    "            # Search for the modified station name in the DataFrame and fill in the missing values\n",
    "            match = df[df['To Station'] == modified_name]\n",
    "            if not match.empty:\n",
    "                df.at[index, 'To UniqueStationName'] = match['To UniqueStationName'].values[0]\n",
    "                df.at[index, 'To PrimaryNaptanStopArea'] = match['To PrimaryNaptanStopArea'].values[0]\n",
    "           \n",
    "            modified_name = row['To Station'].replace(\" EL\", \" DLR\")\n",
    "            # Search for the modified station name in the DataFrame and fill in the missing values\n",
    "            match = df[df['To Station'] == modified_name]\n",
    "            if not match.empty:\n",
    "                df.at[index, 'To UniqueStationName'] = match['To UniqueStationName'].values[0]\n",
    "                df.at[index, 'To PrimaryNaptanStopArea'] = match['To PrimaryNaptanStopArea'].values[0]\n",
    "\n",
    "                \n",
    "        # Check if From UniqueStationName or From PrimaryNaptanStopArea are missing\n",
    "        if pd.isna(row['From UniqueStationName']) or pd.isna(row['From PrimaryNaptanStopArea']):\n",
    "            # Modify the station name by removing \" el\"\n",
    "            modified_name = row['From Station'].replace(\" EL\", \"\")\n",
    "            # Search for the modified station name in the DataFrame and fill in the missing values\n",
    "            match = df[df['From Station'] == modified_name]\n",
    "            if not match.empty:\n",
    "                df.at[index, 'From UniqueStationName'] = match['From UniqueStationName'].values[0]\n",
    "                df.at[index, 'From PrimaryNaptanStopArea'] = match['From PrimaryNaptanStopArea'].values[0]\n",
    "            \n",
    "            modified_name = row['From Station'].replace(\" EL\", \" DLR\")\n",
    "            # Search for the modified station name in the DataFrame and fill in the missing values\n",
    "            match = df[df['From Station'] == modified_name]\n",
    "            if not match.empty:\n",
    "                df.at[index, 'From UniqueStationName'] = match['From UniqueStationName'].values[0]\n",
    "                df.at[index, 'From PrimaryNaptanStopArea'] = match['From PrimaryNaptanStopArea'].values[0]\n",
    "\n",
    "  \n",
    "    # Add the modified DataFrame\n",
    "    data_dict[key] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38fa1587",
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Add Long Lat Data for Elizabeth/DLR stations with same location but different Name\n",
    "\n",
    "for key in data_dict.keys():\n",
    "    df = data_dict.get(key)\n",
    "\n",
    "    # Iterate through the rows of the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if To UniqueStationName, To PrimaryNaptanStopArea, StopLat or StopLong are missing\n",
    "        if pd.isna(row['To UniqueStationName']) or pd.isna(row['To PrimaryNaptanStopArea']) or pd.isna(row['StopLat']) or pd.isna(row['StopLong']):\n",
    "            # Modify the station name by removing \" EL\"\n",
    "            modified_name = row['To Station'].replace(\" EL\", \"\")\n",
    "            # Search for the modified station name in the DataFrame and fill in the missing values\n",
    "            match = df[df['To Station'] == modified_name]\n",
    "            if not match.empty:\n",
    "                df.at[index, 'StopLat'] = match['StopLat'].values[0]\n",
    "                df.at[index, 'StopLong'] = match['StopLong'].values[0]\n",
    "           \n",
    "            # Additional modification to station name\n",
    "            modified_name = row['To Station'].replace(\" EL\", \" DLR\")\n",
    "            # Search for the modified station name in the DataFrame and fill in the missing values\n",
    "            match = df[df['To Station'] == modified_name]\n",
    "            if not match.empty:\n",
    "                df.at[index, 'StopLat'] = match['StopLat'].values[0]\n",
    "                df.at[index, 'StopLong'] = match['StopLong'].values[0]\n",
    "\n",
    "        # Check if From UniqueStationName, From PrimaryNaptanStopArea, StartLat or StartLong are missing\n",
    "        if pd.isna(row['From UniqueStationName']) or pd.isna(row['From PrimaryNaptanStopArea']) or pd.isna(row['StartLat']) or pd.isna(row['StartLong']):\n",
    "            # Modify the station name by removing \" EL\"\n",
    "            modified_name = row['From Station'].replace(\" EL\", \"\")\n",
    "            # Search for the modified station name in the DataFrame and fill in the missing values\n",
    "            match = df[df['From Station'] == modified_name]\n",
    "            if not match.empty:\n",
    "                df.at[index, 'StartLat'] = match['StartLat'].values[0]\n",
    "                df.at[index, 'StartLong'] = match['StartLong'].values[0]\n",
    "            \n",
    "            # Additional modification to station name\n",
    "            modified_name = row['From Station'].replace(\" EL\", \" DLR\")\n",
    "            # Search for the modified station name in the DataFrame and fill in the missing values\n",
    "            match = df[df['From Station'] == modified_name]\n",
    "            if not match.empty:\n",
    "                df.at[index, 'StartLat'] = match['StartLat'].values[0]\n",
    "                df.at[index, 'StartLong'] = match['StartLong'].values[0]\n",
    "\n",
    "    # Store the modified DataFrame back in the dictionary\n",
    "    data_dict[key] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d1ef1",
   "metadata": {},
   "source": [
    "Add coordinates of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "727fe1fa",
   "metadata": {
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [],
   "source": [
    "#In subsequent years do not run this code\n",
    "#This code is not reslient as \"missing\" is hard coded\n",
    "missing = np.array(['Barking Riverside', 'Battersea Power Station', 'Nine Elms', 'Woolwich EL'])\n",
    "\n",
    "#Add the coordinates as values to a dictionary with stations as keys\n",
    "latlongcoords = {}\n",
    "\n",
    "#Add the coordinates of each station as values to a dictionary with the station name as a key\n",
    "for a in missing:\n",
    "    latlongcoords[a] = get_lat_long(a)                   \n",
    "                   \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48379f76",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Use this code if NUMBAT or NAP data is different, this is resilient to change\n",
    "#Add the coordinates as values to a dictionary with stations as keys\n",
    "latlongcoords = {}\n",
    "\n",
    "#Add the coordinates of each station as values to a dictionary with the station name as a key\n",
    "for a in np.unique(missing[\"From Station\"]):\n",
    "    latlongcoords[a] = get_lat_long(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9207808b",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#Iterate ove the data_dict to add missing coordinate data\n",
    "for key in data_dict.keys():\n",
    "    # Get the dataframe corresponding to the current key\n",
    "    df2 = data_dict.get(key)\n",
    "    \n",
    "    for index, row in df2.iterrows():\n",
    "        if row[\"From Station\"] in latlongcoords:\n",
    "            coords = latlongcoords.get(row[\"From Station\"])\n",
    "            df2.loc[index, \"StartLat\"] = coords[0]\n",
    "            df2.loc[index, \"StartLong\"] = coords[1]\n",
    "            \n",
    "        if row[\"To Station\"] in latlongcoords:\n",
    "            coords = latlongcoords.get(row[\"To Station\"])\n",
    "            df2.loc[index, \"StopLat\"] = coords[0]\n",
    "            df2.loc[index, \"StopLong\"] = coords[1]\n",
    "\n",
    "    # Update the value of the current key in the data_dict dictionary with the updated DataFrame\n",
    "    data_dict[key] = df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3792cc",
   "metadata": {},
   "source": [
    "## Get optimal Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95178866",
   "metadata": {},
   "source": [
    "Use NaPTAN codes to make api calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c45c8f1",
   "metadata": {},
   "source": [
    "This has missing values. Those can be filled in manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "238bdbd0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize an empty set to store the unique Naptan Pair values\n",
    "unique_stations = set()\n",
    "\n",
    "# Iterate over the list of filenames and import each file\n",
    "for df in data_dict.values():\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        \n",
    "        a = row[\"From PrimaryNaptanStopArea\"]\n",
    "        b = row[\"To PrimaryNaptanStopArea\"]\n",
    "    \n",
    "        c = a + \"-split-\" + b\n",
    "    # Add the unique values to the set\n",
    "        unique_stations.add(c)\n",
    "\n",
    "# Convert the set to a list or other iterable object if needed\n",
    "unique_list = list(unique_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b416e02f",
   "metadata": {
    "code_folding": [
     0,
     4
    ]
   },
   "outputs": [],
   "source": [
    "#Function to call TFL API for optimal times\n",
    "#Replace app_key with your api key from https://techforum.tfl.gov.uk/t/app-key-where-is-it/1571\n",
    "app_id = 'your_app_id'\n",
    "app_key = 'ef33acb68f494da1bfbfcc0efdadd70a'\n",
    "def get_optimal_travel_time(from_station_name, to_station_name):\n",
    "    from_station_encoded = quote(from_station_name)\n",
    "    to_station_encoded = quote(to_station_name)\n",
    "    \n",
    "    url = f'https://api.tfl.gov.uk/Journey/JourneyResults/{from_station_encoded}/to/{to_station_encoded}?nationalSearch=False&timeIs=Departing&journeyPreference=LeastTime&mode=tube&app_id={app_id}'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    if 'journeys' in data:\n",
    "        journeys = data['journeys']\n",
    "        fastest_journey = min(journeys, key=lambda x: x['duration'])\n",
    "        travel_time = fastest_journey['duration']\n",
    "        return travel_time\n",
    "    \n",
    "    elif 'message' in data and data['message'] == 'No journey found for your inputs.':\n",
    "        print(f\"No journey found between {from_station_name} and {to_station_name}.\")\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        print(\"Error: \")\n",
    "        print(f\"{from_station_name} and {to_station_name}.\")\n",
    "        print(\"data: \", data)\n",
    "        \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c32d7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Create Dictionary of Naptan Pairs and Optimal times\n",
    "optimaltimes = {}\n",
    "\n",
    "for a in unique_list:\n",
    "    time.sleep(10)\n",
    "    word = a.split(\"-split-\")\n",
    "    optimaltimes[a] = get_optimal_travel_time(word[0], word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e675f",
   "metadata": {},
   "source": [
    "## Pickle the Data so it does not need to be recreated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5b015",
   "metadata": {},
   "source": [
    "Pickle data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6beaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First set the directory\n",
    "directoryfilepath = \"~/NUMBAT-main/data\"\n",
    "os.chdir(directoryfilepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e482bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the data\n",
    "with open('datadict.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
